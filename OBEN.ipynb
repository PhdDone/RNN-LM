{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing RNN-LM \"From Scratch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Explain design principles of input/output representations, word embedding, time step\n",
    "structure for feeding hidden states, weight matrix management for feedforward and backpropagation, BPTT(Backpropagation Through Time) algorithm, and optimization(SGD, Adam, or AdaGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer: **\n",
    "The input and output of RNN are vectors. We first build a dictionary and replace the string with index\n",
    "\n",
    "Word embeddings is to map a string word to a vector of numbers, here I use one hot emmbeding. For example if I use a vocab with size 15000, dictionary[\"this\"] = 3, then a vector [0, 0, 0, 1, ...,0, 0] with dim euqals to 5000 will represent the input word \"this\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For RNN we have \n",
    "$$s_{cur} = tanh(Ux_{cur} + Ws_{pre})$$\n",
    "$$o_{cur} = softmax(Vs_{cur})$$\n",
    "where $s_{cur}$ is the current state\n",
    "\n",
    "$s_{prev}$ is the prev state\n",
    "\n",
    "$x_{cur}$ is the current input\n",
    "\n",
    "$U$, $V$, $W$ are the shared parameter matrixes across the time, action as input gate, output gate and write gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment I choose vocab size = 15000, hidden layer size = 100, so we have:\n",
    "\n",
    "intput x.shape = $\\mathbb{R}^{15000}$\n",
    "\n",
    "output o.shape = $\\mathbb{R}^{15000}$\n",
    "\n",
    "state s.shape = $\\mathbb{R}^{100}$\n",
    "\n",
    "input weight matrix $U$.shape = $\\mathbb{R}^{100\\times15000}$\n",
    "\n",
    "output weight matrix $V$.shape = $\\mathbb{R}^{15000\\times100}$\n",
    "\n",
    "write weight matrix $W$.shape = $\\mathbb{R}^{100\\times100}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BPTT, the only different with standard back propagation algorithm is we sum up the gradients for $W$ at each time step, For optimization I use SGD, so in each training iteration I will calculate the gradient of that batch, not the whole dataset to save time. The disadvantage is gradient will may change dramatically from iteration to iteration and hard to decide the learning rate. \n",
    "\n",
    "AdaGrad updates the gradient by combine current gradient with the history gradients. It adapts the learning rate to the parameter. The basic idea is if the sum of history gradients is large(means we already learn a lot on that dimension), the current gradient will contribute less to the weight updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import lib\n",
    "import zipfile                                                                                                                                                            \n",
    "import os                                                                                                                                                \n",
    "import numpy as np                                                                                                                                                        \n",
    "import random                                                                                                                                                             \n",
    "import math                                                                                                                                                               \n",
    "import collections \n",
    "import operator\n",
    "import datetime\n",
    "import sys\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "# Download data\n",
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 17005207\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "def read_data(filename):\n",
    "  \"\"\"Extract the first file enclosed in a zip file as a list of words\"\"\"\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    data = f.read(f.namelist()[0]).split()\n",
    "  return data\n",
    "  \n",
    "words = read_data(filename)\n",
    "print('Data size %d' % len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size 15000\n"
     ]
    }
   ],
   "source": [
    "##### Build vocab and inverse vocab\n",
    "vocabulary_size = 15000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "print('Vocab size %d' % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build training data.\n",
    "# Each training instance is the word in dataset, label is the next word right after the training word\n",
    "data_index = 0\n",
    "\n",
    "def generate_batch(training_size, batch_size):\n",
    "  global data_index\n",
    "\n",
    "  training_data = np.ndarray(shape=(training_size, batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(training_size, batch_size), dtype=np.int32)\n",
    "  buffer = collections.deque(maxlen=batch_size + 1)\n",
    "  for _ in range(batch_size + 1):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(training_size):\n",
    "    list_buffer = list(buffer)\n",
    "    training_data[i] = list_buffer[:batch_size]\n",
    "    labels[i] = list_buffer[1:]\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "    for _ in range(batch_size):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    \n",
    "  return training_data, labels\n",
    "\n",
    "print('data:', [reverse_dictionary[di] for di in data[:8]])\n",
    "\n",
    "number_of_batch = 1000\n",
    "batch_size = 10\n",
    "training_data, labels = generate_batch(training_size=number_of_batch, batch_size = batch_size)\n",
    "print('toaol training size:', len(training_data) * batch_size)\n",
    "print('label size:', len(labels) * batch_size)\n",
    "print('example training batch data:', [[reverse_dictionary[b] for b in bi] for bi in batch[:2]])\n",
    "print('example labels:',[[reverse_dictionary[l] for l in li] for li in labels[:2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0) \n",
    "class RNNNumpy:\n",
    "     \n",
    "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
    "        # Assign instance variables\n",
    "        self.word_dim = word_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bptt_truncate = bptt_truncate\n",
    "        # Randomly initialize the network parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))\n",
    "    \n",
    "    def forward_propagation(self, x):\n",
    "        # The total number of time steps\n",
    "        T = len(x)\n",
    "        # During forward propagation we save all hidden states in s because need them later.\n",
    "        # We add one additional element for the initial hidden, which we set to 0\n",
    "        s = np.zeros((T + 1, self.hidden_dim))\n",
    "        s[-1] = np.zeros(self.hidden_dim)\n",
    "        # The outputs at each time step. Again, we save them for later.\n",
    "        o = np.zeros((T, self.word_dim))\n",
    "        # For each time step...\n",
    "        for t in np.arange(T):\n",
    "            # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
    "            s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
    "            o[t] = softmax(self.V.dot(s[t]))\n",
    "        return [o, s]\n",
    "    \n",
    "    def predict(self, x):\n",
    "        # Perform forward propagation and return index of the highest score\n",
    "        o, s = self.forward_propagation(x)\n",
    "        return np.argmax(o, axis=1)\n",
    "    \n",
    "    def calculate_total_loss(self, x, y):\n",
    "        L = 0\n",
    "        # For each sentence...\n",
    "        for i in np.arange(len(y)):\n",
    "            o, s = self.forward_propagation(x[i])\n",
    "            # We only care about our prediction of the \"correct\" words\n",
    "            correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
    "            # Add to the loss based on how off we were\n",
    "            L += -1 * np.sum(np.log(correct_word_predictions))\n",
    "        return L\n",
    " \n",
    "    def calculate_loss(self, x, y):\n",
    "        # Divide the total loss by the number of training examples\n",
    "        N = np.sum((len(y_i) for y_i in y))\n",
    "        return self.calculate_total_loss(x,y)/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 15000)\n",
      "[[  6.62109333e-05   6.67196519e-05   6.67217783e-05 ...,   6.67782746e-05\n",
      "    6.66192580e-05   6.67231913e-05]\n",
      " [  6.64688245e-05   6.68617663e-05   6.64587208e-05 ...,   6.66221047e-05\n",
      "    6.66575393e-05   6.65015954e-05]\n",
      " [  6.66438125e-05   6.69261478e-05   6.67110563e-05 ...,   6.69424599e-05\n",
      "    6.61166672e-05   6.66485979e-05]\n",
      " ..., \n",
      " [  6.69452754e-05   6.64347871e-05   6.66880558e-05 ...,   6.67230928e-05\n",
      "    6.64866970e-05   6.68044492e-05]\n",
      " [  6.68432246e-05   6.70532657e-05   6.62952360e-05 ...,   6.64221627e-05\n",
      "    6.61275837e-05   6.69638582e-05]\n",
      " [  6.67078676e-05   6.66390377e-05   6.67673229e-05 ...,   6.70055253e-05\n",
      "    6.68277927e-05   6.65578990e-05]]\n"
     ]
    }
   ],
   "source": [
    "model = RNNNumpy(vocabulary_size)\n",
    "o, s = model.forward_propagation(training_data[0]) # input batch_size words\n",
    "print(o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "[ 8384  3893 13076  6075  8203  3196  9363  8397 12661 11314]\n"
     ]
    }
   ],
   "source": [
    "# Try predict, each word is batch[0] will predict a new word\n",
    "predictions = model.predict(training_data[0])\n",
    "print predictions.shape\n",
    "print predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected Loss for random predictions: 9.615805\n",
      "Actual loss: 9.615732\n"
     ]
    }
   ],
   "source": [
    "# Try loss\n",
    "print \"Expected Loss for random predictions: %f\" % np.log(vocabulary_size)\n",
    "print \"Actual loss: %f\" % model.calculate_loss(training_data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bptt(self, x, y):\n",
    "    T = len(y)\n",
    "    # Perform forward propagation\n",
    "    o, s = self.forward_propagation(x)\n",
    "    # We accumulate the gradients in these variables\n",
    "    dLdU = np.zeros(self.U.shape)\n",
    "    dLdV = np.zeros(self.V.shape)\n",
    "    dLdW = np.zeros(self.W.shape)\n",
    "    delta_o = o\n",
    "    delta_o[np.arange(len(y)), y] -= 1.\n",
    "    # For each output backwards...\n",
    "    for t in np.arange(T)[::-1]:\n",
    "        dLdV += np.outer(delta_o[t], s[t].T)\n",
    "        # Initial delta calculation\n",
    "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
    "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
    "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
    "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
    "            dLdU[:,x[bptt_step]] += delta_t\n",
    "            # Update delta for next step\n",
    "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
    "    return [dLdU, dLdV, dLdW]\n",
    " \n",
    "RNNNumpy.bptt = bptt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing gradient check for parameter U with size 1000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.auto/home/ydong/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check for parameter U passed.\n",
      "Performing gradient check for parameter V with size 1000.\n",
      "Gradient check for parameter V passed.\n",
      "Performing gradient check for parameter W with size 100.\n",
      "Gradient check for parameter W passed.\n"
     ]
    }
   ],
   "source": [
    "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
    "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
    "    bptt_gradients = self.bptt(x, y)\n",
    "    # List of all parameters we want to check.\n",
    "    model_parameters = ['U', 'V', 'W']\n",
    "    # Gradient check for each parameter\n",
    "    for pidx, pname in enumerate(model_parameters):\n",
    "        # Get the actual parameter value from the mode, e.g. model.W\n",
    "        parameter = operator.attrgetter(pname)(self)\n",
    "        print \"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape))\n",
    "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
    "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
    "        while not it.finished:\n",
    "            ix = it.multi_index\n",
    "            # Save the original value so we can reset it later\n",
    "            original_value = parameter[ix]\n",
    "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
    "            parameter[ix] = original_value + h\n",
    "            gradplus = self.calculate_total_loss([x],[y])\n",
    "            parameter[ix] = original_value - h\n",
    "            gradminus = self.calculate_total_loss([x],[y])\n",
    "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
    "            # Reset parameter to original value\n",
    "            parameter[ix] = original_value\n",
    "            # The gradient for this parameter calculated using backpropagation\n",
    "            backprop_gradient = bptt_gradients[pidx][ix]\n",
    "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
    "            relative_error = np.abs(backprop_gradient - estimated_gradient)/(np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
    "            # If the error is too large fail the gradient check\n",
    "            if relative_error > error_threshold:\n",
    "                print \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix)\n",
    "                print \"+h Loss: %f\" % gradplus\n",
    "                print \"-h Loss: %f\" % gradminus\n",
    "                print \"Estimated_gradient: %f\" % estimated_gradient\n",
    "                print \"Backpropagation gradient: %f\" % backprop_gradient\n",
    "                print \"Relative Error: %f\" % relative_error\n",
    "                return\n",
    "            it.iternext()\n",
    "        print \"Gradient check for parameter %s passed.\" % (pname)\n",
    " \n",
    "RNNNumpy.gradient_check = gradient_check\n",
    " \n",
    "# To avoid performing millions of expensive calculations we use a smaller vocabulary size for checking.\n",
    "grad_check_vocab_size = 100\n",
    "model = RNNNumpy(grad_check_vocab_size, 10, bptt_truncate=1000)\n",
    "model.gradient_check([0,1,2,3], [1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Performs one step of SGD.\n",
    "def numpy_sdg_step(self, x, y, learning_rate):\n",
    "    # Calculate the gradients\n",
    "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
    "    # Change parameters according to gradients and learning rate\n",
    "    self.U -= learning_rate * dLdU\n",
    "    self.V -= learning_rate * dLdV\n",
    "    self.W -= learning_rate * dLdW\n",
    " \n",
    "RNNNumpy.sgd_step = numpy_sdg_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Outer SGD Loop\n",
    "# - model: The RNN model instance\n",
    "# - X_train: The training data set\n",
    "# - y_train: The training data labels\n",
    "# - learning_rate: Initial learning rate for SGD\n",
    "# - nepoch: Number of times to iterate through the complete dataset\n",
    "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
    "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
    "    # We keep track of the losses so we can plot them later\n",
    "    losses = []\n",
    "    num_examples_seen = 0\n",
    "    for epoch in range(nepoch):\n",
    "        # Optionally evaluate the loss\n",
    "        if (epoch % evaluate_loss_after == 0):\n",
    "            loss = model.calculate_loss(X_train, y_train)\n",
    "            losses.append((num_examples_seen, loss))\n",
    "            time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            print \"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss)\n",
    "            # Adjust the learning rate if loss increases\n",
    "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
    "                learning_rate = learning_rate * 0.5 \n",
    "                print \"Setting learning rate to %f\" % learning_rate\n",
    "            sys.stdout.flush()\n",
    "        # For each training example...\n",
    "        for i in range(len(y_train)):\n",
    "            # One SGD step\n",
    "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
    "            num_examples_seen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-07-02 17:28:30: Loss after num_examples_seen=0 epoch=0: 9.615685\n",
      "2016-07-02 17:30:40: Loss after num_examples_seen=500 epoch=1: 9.606484\n",
      "2016-07-02 17:32:45: Loss after num_examples_seen=1000 epoch=2: 8.597782\n",
      "2016-07-02 17:34:59: Loss after num_examples_seen=1500 epoch=3: 7.280509\n",
      "2016-07-02 17:37:07: Loss after num_examples_seen=2000 epoch=4: 6.920833\n",
      "2016-07-02 17:39:22: Loss after num_examples_seen=2500 epoch=5: 6.749891\n",
      "2016-07-02 17:41:39: Loss after num_examples_seen=3000 epoch=6: 6.634353\n",
      "2016-07-02 17:43:46: Loss after num_examples_seen=3500 epoch=7: 6.565943\n",
      "2016-07-02 17:45:59: Loss after num_examples_seen=4000 epoch=8: 6.502040\n",
      "2016-07-02 17:48:09: Loss after num_examples_seen=4500 epoch=9: 6.440839\n"
     ]
    }
   ],
   "source": [
    "# train a small model with 500 training examples\n",
    "training_size = 500\n",
    "model = RNNNumpy(vocabulary_size)\n",
    "losses = train_with_sgd(model, training_data[:training_size], labels[:training_size], nepoch=10, evaluate_loss_after=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate next words given history words\n",
    "def generate_sentence(model, start_word, sent_len):\n",
    "    # We start the sentence with the start token\n",
    "    new_sentence = [start_word]\n",
    "    # Repeat until we get an the sent len we want\n",
    "    while not len(new_sentence) == sent_len:\n",
    "        next_word_probs, s = model.forward_propagation(new_sentence)\n",
    "        sampled_word = np.argmax(next_word_probs[-1])\n",
    "        new_sentence.append(sampled_word)\n",
    "    sentence_str = [reverse_dictionary[x] for x in new_sentence]\n",
    "    return sentence_str\n",
    "num_sentences = 20\n",
    "sent_min_length = 5\n",
    "# Randomly got 10 words from vocab as start word\n",
    "np.random.seed(10)\n",
    "initial_words = np.random.random_integers(0, vocabulary_size, (num_sentences))\n",
    "for i in range(num_sentences):\n",
    "    sent = []\n",
    "    # We want long sentences, not sentences with one or two words\n",
    "    sent = generate_sentence(model, initial_words[i], sent_min_length)\n",
    "    print \" \".join(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "When you want to apply LSTM(Long­Short Term Memory) instead of vanilla RNN, what would you consider more in terms of cell and hidden states, and several gates?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** answer: **\n",
    "\n",
    "LSTM network is designed to deal with vanishing gradients through a gating mechanism. Basically there will be a *input gate*, *output gate*, and *forget gate*. They are matrixes with value between 0 to 1. \n",
    "\n",
    "- the forget gate defines how much of the previous state you want to let through\n",
    "- the input gate defines how much of the newly computed state for current input youwant to let through\n",
    "- the output gate defines how much of the internal state you want to expose to the external network\n",
    "\n",
    "Given the current input and prev state, a cell can calculate the current hidden state. All gates has the same dimentions with the size of hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "If you try to use the pretrained word2vec, what parts in your structure have to be changed, and what will be the expected effect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** answer: **\n",
    "\n",
    "If we want to use a pre-trained word embedding, we can change th one hot vocab to word embedding vocab. The current\n",
    "*dictionary* will change to a map from word string to word embedding vector, the *reverse_dictionary* will be a map from word embedding vector to a string. Use a pre-trained word embedding is useful when the training data is small.\n",
    "\n",
    "If we don't want to use a pre-trained one, we can also add a embedding layer between the one hot word vector and hidden layer. By update the embedding layer during training, we can also learn a word vectors, but it will be less general.\n",
    "\n",
    "By using the pretrained word vector, we will expect faster updating speed and lower loss, because now we know the relationship between each words, the model can learn one word and generalize to the other similar words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "What could you expect when dropout is applied to feedforward? How would you implement it?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** answer: **\n",
    "\n",
    "Dropout can be applied to avoid overfitting. Dropout is applied to feedforword in RNNs and the recurrent connections are kept untouched. \n",
    "\n",
    "To implement it, we need to randomly drop the output from hidden layer to output layer. In order to do that, for the output matrix V, during training, if we set the dropout rate to be 0.5, everytime we need to randomly select half the the rows in V and set them to be zero. So those hidden layer correspond to those rows will not affect the output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "When it comes to prediction, could you consider beam search? What else could you suggest to dynamically generate sentences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, beam search is a heuristic search algorithm to find a sub optimal when it's impossible/hard to find a global optimal. For a k-beam search, we keep k candidates with largest scores in the beam, everytime we do a prediction for every candidate in the beam and choose the largest k candidates and replace the old ones in the beam. In this way we can explore more states then greedy search. Gready search is actully a beam search with beam size euqals 1.\n",
    "\n",
    "Other way could be randomly select one or more next state based on their scores/probs. This will make the results more diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/) RECURRENT NEURAL NETWORKS TUTORIAL, PART 2  IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO\n",
    "\n",
    "[2](https://www.tensorflow.org/versions/r0.9/tutorials/recurrent/index.html) Recurrent Neural Networks - Tensorflow\n",
    "\n",
    "[3](http://sebastianruder.com/optimizing-gradient-descent/)An overview of gradient descent optimization algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
